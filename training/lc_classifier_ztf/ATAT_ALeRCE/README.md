# ATAT-ALeRCE

ATAT: Astronomical Transformer for time series And Tabular data consists of two Transformer models that encode light curves and features using novel Time Modulation (TM) and Quantile Feature Tokenizer (QFT) mechanisms, respectively.

<img src="https://arxiv.org/html/2405.03078v2/x1.png" alt="Description of the image" style="background-color: white; padding: 10px;">

**Code authors**:
- Nicolas Astorga 
- Bastian Gamboa
- Daniel Moreno

## Main code scripts

```
ðŸ“¦ATAT-Refactorized

# Configurations used in training
 â”£ ðŸ“‚ configs: Contains the hyperparameter configurations.
 â”ƒ â”— ðŸ“œ training.yaml: A dictionary specifying the components (LC, MD, Feat, MTA, QT) to be used by ATAT.

# Data
 â”£ ðŸ“‚ data:
 â”ƒ â”£ ðŸ“‚ datasets:
 â”ƒ â”ƒ â”£ ðŸ“‚ ZTF_ff: (ZTF forced photometry)
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ final: Contains the final dataset, quantiles, and a dictionary with data information.
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ quantiles
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“œ dataset.h5
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“œ dict_info.yaml: Contains critical information about dataset creation used during training.
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ partitions: Contains versions of the data partitions.
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ processed: 
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ data_231206: Light curves in parquet format derived from raw data using extracting_ztf_md_feat.
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ md_feat_231206_v2
 â”ƒ â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ metadata: Metadata in parquet format derived from raw data using extracting_ztf_md_feat.
 â”ƒ â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ features: Features per day in parquet format derived from raw data using extracting_ztf_md_feat.
 â”ƒ â”ƒ â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ 16_days: Features calculated on day 16.
 â”ƒ â”ƒ â”ƒ â”ƒ â”ƒ â”ƒ â”£ ...  
 â”ƒ â”ƒ â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“‚ 2048_days: Features calculated on day 2048.
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“‚ ulens_keep: Contains CSVs of the ulens that were effectively used.
 â”ƒ â”ƒ â”ƒ â”— ðŸ“‚ raw 
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ data_231206_ao: Batched light curves of astro_objects.
 â”ƒ â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ data_231206_ao_features: Batched features of astro_objects.
 â”ƒ â”ƒ â”ƒ â”ƒ â”— ðŸ“‚ pipeline: ALeRCE pipeline necessary for extracting information from astro_objects files.
 â”ƒ â”ƒ â”£ ðŸ“‚ ELASTICC_2: (The Extended LSST Astronomical Time-Series Classification Challenge) **[IN PROCESS]**
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ final 
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ partitions
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ processed 
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ raw 
 â”ƒ â”£ ðŸ“‚ src: Contains the .py files for ATAT processing, and for generating partitions and datasets. 
 â”ƒ â”£ ðŸ“œ ZTF_ff_processed_to_final.py: Generates the final ZTF dataset using the processed data.
 â”ƒ â”— ðŸ“œ ZTF_ff_raw_to_processed.py: Converts raw ZTF data into processed form.

# Visualization
 â”£ ðŸ“‚ notebooks **[IN PROCESS]**
 â”ƒ â”£ ðŸ“‚ data_exploration 
 â”ƒ â”£ ðŸ“‚ meta_model 
 â”ƒ â”£ ðŸ“‚ model_exploration
 â”ƒ â”— ðŸ“‚ vis_results 
 â”ƒ â”ƒ â”£ ðŸ“œ classification_metrics.ipynb 
 â”ƒ â”ƒ â”£ ðŸ“œ confusion_matrices.ipynb 
 â”ƒ â”ƒ â”— ðŸ“œ plot_metrics_times.ipynb 

# Training
 â”£ ðŸ“‚ src: 
 â”ƒ â”£ ðŸ“‚ data:
 â”ƒ â”ƒ â”£ ðŸ“‚ handlers: Contains the dataloaders for the training process.
 â”ƒ â”ƒ â”£ ðŸ“‚ modules: Contains the Lightning data modules for training.
 â”ƒ â”£ ðŸ“‚ layers 
 â”ƒ â”ƒ â”£ ðŸ“‚ classifiers: Contains the lc, tab, and mix classifiers.
 â”ƒ â”ƒ â”£ ðŸ“‚ embeddings: Embeddings used for tabular data.
 â”ƒ â”ƒ â”£ ðŸ“‚ timeEncoders: Contains positional encodings and the mixture between magnitudes and times (LC transformer input).
 â”ƒ â”ƒ â”£ ðŸ“‚ tokenEmbeddings: Contains the cls token.
 â”ƒ â”ƒ â”£ ðŸ“‚ transformer: Contains the attention blocks and multi-head attention.
 â”ƒ â”ƒ â”ƒ â”£ ðŸ“‚ mha
 â”ƒ â”ƒ â”£ ðŸ“‚ utils
 â”ƒ â”ƒ â”— ðŸ“œ ATAT.py: Contains the ATAT model.
 â”ƒ â”£ ðŸ“‚ models: Contains the Lightning module for training.
 â”ƒ â”£ ðŸ“‚ training: Contains utility functions for the training process.
 â”ƒ â”— ðŸ“‚ utils: Contains various plotting functions.

# Run the Training and Inference
 â”£ ðŸ“œ custom_parser.py
 â”£ ðŸ“œ training.py
 â”— ðŸ“œ inference_ztf.py 
 ```

The steps to run ATAT are the following:

```
git clone https://github.com/alercebroker/pipeline.git
cd pipeline/training/lc_classifier_ztf/ATAT_ALeRCE
```

## Environment Setup

Firstly, you should create the enviroment:

- conda create -n ATAT python==3.10.10
- conda activate ATAT
- pip install -r requirements.txt

## Data Acquisition 

To train ATAT you will need the following data:
* **pickle files to load instances of the AstroObject class**. ALeRCE classifiers use the AstroObject class to handle data including features, stamps, light curves, etc. AstroObject instances can be stored and loaded from pickle files that contain a list of dictionaries. Each of these ditionaries correspond to one astronomical object. Each dictionary includes: 'metadata', 'detections', 'non_detections', 'forced_photometry', 'xmatch', 'stamps', 'features', 'predictions'. A set of chunks are stored in multiple pickle files.
* **parquet files containing single light curves (e.g. detections + forced photometry) per object**. The AstroObject files need to be processed in order to add detections and forced photometry into single light curves. This is done by using ZTF_ff_raw_to_processed.py which combines the light curves in the the AstroObject pickle files and store them into parquet files.
* **microlensing events to be removed**. Some spurious microlensing events were detected by the ALeRCE team. These are stored in csv files that only contain the object IDs of objects to be removed/kept.
* **parquet files containing partitions**. We use K-fold cross validation. In order to have exactly the same train-validation-test sets for each partition we define a parquet file called partitions.parquet which contains the columns 'oid' (object ID), 'alerceclass' (the label of that object), ra', 'dec', and 'partition'. The latest can be one of the following: 'test', 'training_0', 'training_1', 'training_2', 'training_3', 'training_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4'. If 'test' that means that object is never used for validation or training. If 'training_i' it means that the object is used for training in the $i$-th experiment, while 'validation_i' means the object is used for validation in the $i$-th experiment.


The raw ZTF_ff data is stored at `quimal-cpu2` in `/home/db_storage/ztf_forced_photometry`. Ensure to replicate the same file structure locally: 

```
mkdir -p ./data/datasets/ZTF_ff/raw
scp -r <user_name>@146.83.185.161:/home/db_storage/ztf_forced_photometry/data_231206_ao ./data/datasets/ZTF_ff/raw 
scp -r <user_name>@146.83.185.161:/home/db_storage/ztf_forced_photometry/data_231206_ao_features ./data/datasets/ZTF_ff/raw
```

```
mkdir -p ./data/datasets/ZTF_ff/processed
scp -r <user_name>@146.83.185.161:/home/db_storage/ztf_forced_photometry/data_231206 ./data/datasets/ZTF_ff/processed 
```

Then, run the following to obtain the processed data:

```
python data/ZTF_ff_raw_to_processed.py
```

Additionally, retrieve the mislabeled ulens and those that are to be retained:

```
scp -r <user_name>@146.83.185.161:/home/db_storage/ztf_forced_photometry/ulens_keep ./data/datasets/ZTF_ff/processed 
```

## Generate Partitions and Final Dataset

The partitions used for training the models can be found here:
```
scp -r <user_name>@146.83.185.161:/home/db_storage/ztf_forced_photometry/partitions ./data/datasets/ZTF_ff 
```

Alternatively, you can utilize a previously created partition or assign a new one by setting the path in the path_save_k_fold variable within the ZTF_ff_processed_to_final.py script. If the specified partition does not exist, it will be created automatically. To generate both the partitions and the final dataset, run:

```
python data/ZTF_ff_processed_to_final.py
```

The quantiles are generated in this same script.

## Training

Execute the following commands, depending on the training components (this will generate a `results` file):

* Use the `configs/training.yaml`  to select which components of ATAT to train (only LC, LC + MD, LC + MD + Feat + MTA, etc...). It can be done specifying the hyperparameter `--experiment_type_general` as shown below.

```
# Train only with light curves (LC + MTA)
python training.py --experiment_type_general lc_mta --experiment_name_general experiment_0 --name_dataset_general ztf_ff --data_root_general data/datasets/ZTF_ff/final/LC_MD_FEAT_v3_windows_200_12

.
.
.

# Train with light curves and tabular information together (LC + MD + Feat + MTA)
python training.py --experiment_type_general lc_md_feat_mta --experiment_name_general experiment_0 --name_dataset_general ztf_ff --data_root_general data/datasets/ZTF_ff/final/LC_MD_FEAT_v3_windows_200_12
```

In the hyperparameter `--experiment_name_general` you can put any name you want. You could use `custom_parser.py` to configure the model hyperparameters.

## Evaluating Performance [ Estoy ordenando los Notebooks ]

After training the models, obtain the predictions and evaluate performance over various days (e.g., 16, 32, 64, 128, 256, 512, 1024, and 2048 days) using:

```
python inference_ztf.py ztf_ff
```

This generates a file called `predictions_times.pt` within each trained model's files. You could use the `notebooks/vis_results/ATAT_results_windows_v3.ipynb` to understand how obtain the metrics. It is just a notebook example that can variety depeding on you processing used (e.g. windows, logspace, linspace, etc...).

